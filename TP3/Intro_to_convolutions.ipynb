{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of convolutions with tensorflow\n",
    "\n",
    "In this notebook, you'll be using tensorflow to build a Convolutional Neural Network (CNN).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "\n",
    "Both, [this notebook](https://nbviewer.jupyter.org/github/marc-moreaux/Deep-Learning-classes/blob/master/notebooks/Convolution.ipynb) and this [wikipedia page](https://en.wikipedia.org/wiki/Convolution) might help you understand what is a convolution.\n",
    "\n",
    "no, if we consider two functions $f$ and $g$ taking values from $\\mathbb{Z} \\to \\mathbb{R}$ then:  \n",
    "$ (f * g)[n] = \\sum_{m = -\\infty}^{+\\infty} f[m] \\cdot g[n - m] $\n",
    "\n",
    "In our case, we consider the two vectors $x$ and $w$ :  \n",
    "$ x = (x_1, x_2, ..., x_{n-1}, x_n) $  \n",
    "$ w = (w_1, w_2) $\n",
    "\n",
    "And get :   \n",
    "$ x * w = (w_1 x_1 + w_2 x_2, w_1 x_2 + w_2 x_3, ..., w_1 x_{n-1} + w_2 x_n)$\n",
    "\n",
    "\n",
    "#### Deep learning subtility :\n",
    "    \n",
    "In most of deep learning framewoks, you'll get to chose in between three paddings:\n",
    "- **Same**: $(f*g)$ has the same shape as x (we pad the entry with zeros)\n",
    "- **valid**: $(f*g)$ has the shape of x minus the shape of w plus 1 (no padding on x)\n",
    "- **Causal**: $(f*g)(n_t)$ does not depend on any $(n_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow\n",
    "\n",
    "\"TensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and also used for machine learning applications such as neural networks.[3] It is used for both research and production at Google often replacing its closed-source predecessor, DistBelief.\" - Wikipedia\n",
    "\n",
    "We'll be using tensorflow to build the models we want to use. \n",
    "\n",
    "Here below, we build a AND gate with a very simple neural network :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00839782]\n",
      " [ 0.1499088 ]\n",
      " [ 0.1499088 ]\n",
      " [ 0.78595555]] Tensor(\"Y:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define our Dataset\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([0,0,0,1]).reshape(-1,1)\n",
    "\n",
    "# Define the tensorflow tensors\n",
    "x = tf.placeholder(tf.float32, [None, 2], name='X')  # inputs\n",
    "y = tf.placeholder(tf.float32, [None, 1], name='Y')  # outputs\n",
    "W = tf.Variable(tf.zeros([2, 1]), name='W')\n",
    "b = tf.Variable(tf.zeros([1,]), name='b')\n",
    "\n",
    "# Define the model\n",
    "pred = tf.nn.sigmoid(tf.matmul(x, W) + b)  # Model\n",
    "\n",
    "# Define the loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred) + (1-y) * tf.log(1-pred), reduction_indices=1))\n",
    "\n",
    "# Define the optimizer method you want to use\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "# Include some Tensorboard visualization\n",
    "writer_train = tf.summary.FileWriter(\"./my_model/\")\n",
    "\n",
    "\n",
    "# Start training session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer_train.add_graph(sess.graph)\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        _, c, p = sess.run([optimizer, loss, pred], feed_dict={x: X,\n",
    "                                                      y: Y})\n",
    "print(p, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the graph you just created, launch tensorbord.  \n",
    "`$tensorboard --logdirs=./` on linux (with corresponding logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Get inspiration from the preceding code to build a XOR gate\n",
    "\n",
    "Design a neural network with 2 layers.\n",
    "- layer1 has 2 neurons (sigmoid or tanh activation)\n",
    "- Layer2 has 1 neuron (it outouts the prediction)\n",
    "\n",
    "And train  it\n",
    "\n",
    "It's **mandatory** that you get a **tensorboard visualization** of your graph, try to make it look good, plz :)\n",
    "\n",
    "Here below I put a graph of the model you want to have (yet your weights won't be the same)\n",
    "![graph](https://i.stack.imgur.com/nRZ6z.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00197501]\n",
      " [ 0.99865156]\n",
      " [ 0.49910623]\n",
      " [ 0.50053406]] Tensor(\"Y:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "### Code here\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define our Dataset\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([0,1,1,0]).reshape(-1,1)\n",
    "\n",
    "# Define the tensorflow tensors\n",
    "x = tf.placeholder(tf.float32, [None, 2], name='X')  # inputs\n",
    "y = tf.placeholder(tf.float32, [None, 1], name='Y')  # outputs\n",
    "W1 = tf.Variable(tf.random_normal([2, 2]), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([2,]), name='b1')\n",
    "W2 = tf.Variable(tf.random_normal([2, 1]), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([1,]), name='b2')\n",
    "\n",
    "# Define the model\n",
    "A2 = tf.nn.tanh(tf.matmul(x, W1) + b1)  # Model\n",
    "pred = tf.nn.sigmoid(tf.matmul(A2, W2) + b2)\n",
    "\n",
    "# Define the loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred) + (1-y) * tf.log(1-pred), reduction_indices=1))\n",
    "\n",
    "# Define the optimizer method you want to use\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "# Include some Tensorboard visualization\n",
    "writer_train = tf.summary.FileWriter(\"./my_modelXOR/\")\n",
    "\n",
    "\n",
    "# Start training session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer_train.add_graph(sess.graph)\n",
    "    \n",
    "    for epoch in range(10000):\n",
    "        _, c, p = sess.run([optimizer, loss, pred], feed_dict={x: X,\n",
    "                                                      y: Y})\n",
    "    print_W1 = sess.run(tf.trainable_variables(\"W1\"))\n",
    "    print_W2 = sess.run(tf.trainable_variables(\"W2\"))\n",
    "print(p, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the weights of your model\n",
    "And give an interpretation on what they are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 [array([[ 4.84613419,  4.65626192],\n",
      "       [-2.7323525 ,  2.09388518]], dtype=float32)]\n",
      "W2 [array([[-4.24455166],\n",
      "       [ 3.93683577]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "### Code here\n",
    "print(\"W1\", print_W1)\n",
    "print(\"W2\", print_W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Build a CNN to predict the MNIST digits\n",
    "You can now move to CNNs. You'll have to train a convolutional neural network to predict the digits from MNIST.\n",
    "\n",
    "You might want to reuse some pieces of code from [SNN](https://nbviewer.jupyter.org/github/marc-moreaux/Deep-Learning-classes/blob/master/notebooks/Intro_to_SNN.ipynb)\n",
    "\n",
    "Your model should have 3 layers:\n",
    "- 1st layer : 6 convolutional kernels with shape (3,3)\n",
    "- 2nd layer : 6 convolutional kernels with shape (3,3)\n",
    "- 3rd layer : Softmax layer\n",
    "\n",
    "Train your model.\n",
    "\n",
    "Explain all you do, and why, make it lovely to read, plz o:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Importation of data, instead of downloading via an external source, we use the data included in Tensorflow\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.random_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 0 training accuracy : 0.48 loss : 0.0455452489853\n",
      "step : 100 training accuracy : 0.9 loss : 0.0073886090517\n",
      "step : 200 training accuracy : 0.82 loss : 0.0191392862797\n",
      "step : 300 training accuracy : 0.9 loss : 0.00964303135872\n",
      "step : 400 training accuracy : 0.88 loss : 0.00614393174648\n",
      "step : 500 training accuracy : 0.92 loss : 0.00987895131111\n",
      "step : 600 training accuracy : 0.96 loss : 0.004123685956\n",
      "step : 700 training accuracy : 0.94 loss : 0.00504373311996\n",
      "step : 800 training accuracy : 0.9 loss : 0.00747117459774\n",
      "step : 900 training accuracy : 0.88 loss : 0.00853220820427\n",
      "step : 1000 training accuracy : 0.82 loss : 0.00909156858921\n",
      "step : 1100 training accuracy : 0.96 loss : 0.00326681941748\n",
      "step : 1200 training accuracy : 0.9 loss : 0.00616743266582\n",
      "step : 1300 training accuracy : 0.92 loss : 0.00850886523724\n",
      "step : 1400 training accuracy : 0.92 loss : 0.00566857814789\n",
      "step : 1500 training accuracy : 0.84 loss : 0.0104892086983\n",
      "step : 1600 training accuracy : 0.92 loss : 0.00612631142139\n",
      "step : 1700 training accuracy : 0.92 loss : 0.0058874887228\n",
      "step : 1800 training accuracy : 0.92 loss : 0.0097248595953\n",
      "step : 1900 training accuracy : 0.9 loss : 0.00997936785221\n",
      "step : 2000 training accuracy : 0.92 loss : 0.00965087592602\n",
      "step : 2100 training accuracy : 0.88 loss : 0.00767821490765\n",
      "step : 2200 training accuracy : 0.88 loss : 0.0107884716988\n",
      "step : 2300 training accuracy : 0.94 loss : 0.00798046350479\n",
      "step : 2400 training accuracy : 0.96 loss : 0.00696996629238\n",
      "step : 2500 training accuracy : 0.94 loss : 0.00537337183952\n",
      "step : 2600 training accuracy : 0.92 loss : 0.00509115755558\n",
      "step : 2700 training accuracy : 0.94 loss : 0.00497374385595\n",
      "step : 2800 training accuracy : 0.9 loss : 0.00789600610733\n",
      "step : 2900 training accuracy : 0.88 loss : 0.00638947963715\n",
      "step : 3000 training accuracy : 0.94 loss : 0.00388336747885\n",
      "step : 3100 training accuracy : 0.92 loss : 0.00336467504501\n",
      "step : 3200 training accuracy : 0.94 loss : 0.00578091025352\n",
      "step : 3300 training accuracy : 0.9 loss : 0.0078024828434\n",
      "step : 3400 training accuracy : 0.92 loss : 0.00629646480083\n",
      "step : 3500 training accuracy : 0.94 loss : 0.00724451899529\n",
      "step : 3600 training accuracy : 0.9 loss : 0.00454086303711\n",
      "step : 3700 training accuracy : 0.86 loss : 0.0112138056755\n",
      "step : 3800 training accuracy : 0.86 loss : 0.0107593917847\n",
      "step : 3900 training accuracy : 0.92 loss : 0.00666566133499\n",
      "step : 4000 training accuracy : 0.98 loss : 0.00149796262383\n",
      "step : 4100 training accuracy : 0.94 loss : 0.00620246767998\n",
      "step : 4200 training accuracy : 0.92 loss : 0.005703971982\n",
      "step : 4300 training accuracy : 0.9 loss : 0.00629675865173\n",
      "step : 4400 training accuracy : 0.92 loss : 0.00473917871714\n",
      "step : 4500 training accuracy : 0.96 loss : 0.00401934653521\n",
      "step : 4600 training accuracy : 0.94 loss : 0.00418630599976\n",
      "step : 4700 training accuracy : 0.9 loss : 0.00650525867939\n",
      "step : 4800 training accuracy : 0.9 loss : 0.00579676747322\n",
      "step : 4900 training accuracy : 0.94 loss : 0.00516026556492\n",
      "step : 5000 training accuracy : 0.92 loss : 0.00825526893139\n",
      "step : 5100 training accuracy : 0.96 loss : 0.00528144419193\n",
      "step : 5200 training accuracy : 0.92 loss : 0.00794781506062\n",
      "step : 5300 training accuracy : 0.86 loss : 0.00621524333954\n",
      "step : 5400 training accuracy : 0.94 loss : 0.00768835604191\n",
      "step : 5500 training accuracy : 0.98 loss : 0.00318178743124\n",
      "step : 5600 training accuracy : 0.96 loss : 0.00308034956455\n",
      "step : 5700 training accuracy : 0.9 loss : 0.0081525850296\n",
      "step : 5800 training accuracy : 0.92 loss : 0.0109792625904\n",
      "step : 5900 training accuracy : 0.9 loss : 0.00621987938881\n",
      "step : 6000 training accuracy : 0.88 loss : 0.00965907037258\n",
      "step : 6100 training accuracy : 0.9 loss : 0.00577125489712\n",
      "step : 6200 training accuracy : 0.9 loss : 0.00509496569633\n",
      "step : 6300 training accuracy : 0.92 loss : 0.00689978718758\n",
      "step : 6400 training accuracy : 0.88 loss : 0.00729586720467\n",
      "step : 6500 training accuracy : 0.92 loss : 0.0106461691856\n",
      "step : 6600 training accuracy : 0.94 loss : 0.00803318083286\n",
      "step : 6700 training accuracy : 0.92 loss : 0.0105576920509\n",
      "step : 6800 training accuracy : 0.96 loss : 0.00381375700235\n",
      "step : 6900 training accuracy : 0.94 loss : 0.00664675652981\n",
      "step : 7000 training accuracy : 0.88 loss : 0.0113212275505\n",
      "step : 7100 training accuracy : 0.98 loss : 0.00170615121722\n",
      "step : 7200 training accuracy : 0.96 loss : 0.00255878180265\n",
      "step : 7300 training accuracy : 0.94 loss : 0.00483171105385\n",
      "step : 7400 training accuracy : 0.92 loss : 0.00839440822601\n",
      "step : 7500 training accuracy : 0.92 loss : 0.00551932394505\n",
      "step : 7600 training accuracy : 0.9 loss : 0.00685997247696\n",
      "step : 7700 training accuracy : 0.96 loss : 0.00486276477575\n",
      "step : 7800 training accuracy : 0.96 loss : 0.00320894122124\n",
      "step : 7900 training accuracy : 0.92 loss : 0.0073220705986\n",
      "step : 8000 training accuracy : 0.9 loss : 0.0137744450569\n",
      "step : 8100 training accuracy : 0.9 loss : 0.00762796103954\n",
      "step : 8200 training accuracy : 0.92 loss : 0.00493970304728\n",
      "step : 8300 training accuracy : 0.96 loss : 0.0055768686533\n",
      "step : 8400 training accuracy : 0.94 loss : 0.00346637964249\n",
      "step : 8500 training accuracy : 0.92 loss : 0.00823477089405\n",
      "step : 8600 training accuracy : 0.92 loss : 0.0142531871796\n",
      "step : 8700 training accuracy : 0.9 loss : 0.00531535506248\n",
      "step : 8800 training accuracy : 0.96 loss : 0.00295585930347\n",
      "step : 8900 training accuracy : 0.96 loss : 0.0037325027585\n",
      "step : 9000 training accuracy : 0.92 loss : 0.00495724022388\n",
      "step : 9100 training accuracy : 0.96 loss : 0.00454009354115\n",
      "step : 9200 training accuracy : 0.96 loss : 0.00519333541393\n",
      "step : 9300 training accuracy : 0.96 loss : 0.0023846539855\n",
      "step : 9400 training accuracy : 0.92 loss : 0.00523241221905\n",
      "step : 9500 training accuracy : 0.96 loss : 0.00548707485199\n",
      "step : 9600 training accuracy : 0.94 loss : 0.00315423071384\n",
      "step : 9700 training accuracy : 0.9 loss : 0.00718949556351\n",
      "step : 9800 training accuracy : 0.9 loss : 0.0085942029953\n",
      "step : 9900 training accuracy : 0.94 loss : 0.00348446041346\n",
      "step : 10000 training accuracy : 0.96 loss : 0.0149899733067\n",
      "step : 10100 training accuracy : 0.92 loss : 0.00285390049219\n",
      "step : 10200 training accuracy : 0.9 loss : 0.00502262890339\n",
      "step : 10300 training accuracy : 0.86 loss : 0.00548340857029\n",
      "step : 10400 training accuracy : 0.94 loss : 0.00419643908739\n",
      "step : 10500 training accuracy : 0.94 loss : 0.0115854167938\n",
      "step : 10600 training accuracy : 0.92 loss : 0.00499125629663\n",
      "step : 10700 training accuracy : 0.9 loss : 0.00519717156887\n",
      "step : 10800 training accuracy : 0.86 loss : 0.0203269076347\n",
      "step : 10900 training accuracy : 0.96 loss : 0.00218092158437\n",
      "step : 11000 training accuracy : 0.94 loss : 0.00415833592415\n",
      "step : 11100 training accuracy : 0.96 loss : 0.00462928414345\n",
      "step : 11200 training accuracy : 0.9 loss : 0.00608045220375\n",
      "step : 11300 training accuracy : 0.96 loss : 0.00350061953068\n",
      "step : 11400 training accuracy : 0.94 loss : 0.00713269174099\n",
      "step : 11500 training accuracy : 0.98 loss : 0.00619628250599\n",
      "step : 11600 training accuracy : 0.9 loss : 0.00773733317852\n",
      "step : 11700 training accuracy : 0.92 loss : 0.00510928869247\n",
      "step : 11800 training accuracy : 0.88 loss : 0.00863941907883\n",
      "step : 11900 training accuracy : 0.92 loss : 0.00767747342587\n",
      "step : 12000 training accuracy : 0.88 loss : 0.00822586536407\n",
      "step : 12100 training accuracy : 0.98 loss : 0.00439333349466\n",
      "step : 12200 training accuracy : 0.96 loss : 0.00641345620155\n",
      "step : 12300 training accuracy : 0.94 loss : 0.00438529133797\n",
      "step : 12400 training accuracy : 0.92 loss : 0.00325666040182\n",
      "step : 12500 training accuracy : 0.94 loss : 0.00409989476204\n",
      "step : 12600 training accuracy : 0.88 loss : 0.00831089735031\n",
      "step : 12700 training accuracy : 0.94 loss : 0.00658620655537\n",
      "step : 12800 training accuracy : 0.92 loss : 0.00665540337563\n",
      "step : 12900 training accuracy : 0.94 loss : 0.00751649856567\n",
      "step : 13000 training accuracy : 0.88 loss : 0.0113677883148\n",
      "step : 13100 training accuracy : 0.96 loss : 0.00292975515127\n",
      "step : 13200 training accuracy : 0.86 loss : 0.00925434589386\n",
      "step : 13300 training accuracy : 0.94 loss : 0.00342818766832\n",
      "step : 13400 training accuracy : 0.84 loss : 0.01117156744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 13500 training accuracy : 0.94 loss : 0.00265745341778\n",
      "step : 13600 training accuracy : 0.78 loss : 0.0156340074539\n",
      "step : 13700 training accuracy : 0.96 loss : 0.0019284543395\n",
      "step : 13800 training accuracy : 0.92 loss : 0.00653774738312\n",
      "step : 13900 training accuracy : 0.94 loss : 0.00685636878014\n",
      "step : 14000 training accuracy : 0.98 loss : 0.00187950193882\n",
      "step : 14100 training accuracy : 0.98 loss : 0.00820836484432\n",
      "step : 14200 training accuracy : 0.92 loss : 0.0102838206291\n",
      "step : 14300 training accuracy : 0.96 loss : 0.00510863900185\n",
      "step : 14400 training accuracy : 0.9 loss : 0.0058442735672\n",
      "step : 14500 training accuracy : 0.92 loss : 0.0035718023777\n",
      "step : 14600 training accuracy : 0.92 loss : 0.00662855505943\n",
      "step : 14700 training accuracy : 0.94 loss : 0.00316259086132\n",
      "step : 14800 training accuracy : 0.98 loss : 0.0025366383791\n",
      "step : 14900 training accuracy : 0.94 loss : 0.00543116152287\n",
      "step : 15000 training accuracy : 0.9 loss : 0.0102091228962\n",
      "step : 15100 training accuracy : 0.9 loss : 0.00925436258316\n",
      "step : 15200 training accuracy : 0.92 loss : 0.0131943416595\n",
      "step : 15300 training accuracy : 0.92 loss : 0.00871398150921\n",
      "step : 15400 training accuracy : 0.9 loss : 0.00689134955406\n",
      "step : 15500 training accuracy : 0.98 loss : 0.00160166889429\n",
      "step : 15600 training accuracy : 0.9 loss : 0.00661938428879\n",
      "step : 15700 training accuracy : 0.94 loss : 0.00539519309998\n",
      "step : 15800 training accuracy : 0.92 loss : 0.00270819425583\n",
      "step : 15900 training accuracy : 0.94 loss : 0.00738275706768\n",
      "step : 16000 training accuracy : 0.96 loss : 0.00972534000874\n",
      "step : 16100 training accuracy : 0.94 loss : 0.0098577696085\n",
      "step : 16200 training accuracy : 0.94 loss : 0.00345866858959\n",
      "step : 16300 training accuracy : 0.98 loss : 0.00436548322439\n",
      "step : 16400 training accuracy : 0.94 loss : 0.00766319990158\n",
      "step : 16500 training accuracy : 0.9 loss : 0.00654661536217\n",
      "step : 16600 training accuracy : 0.86 loss : 0.0121750020981\n",
      "step : 16700 training accuracy : 0.92 loss : 0.00337903439999\n",
      "step : 16800 training accuracy : 0.96 loss : 0.00482902228832\n",
      "step : 16900 training accuracy : 0.94 loss : 0.00627040207386\n",
      "step : 17000 training accuracy : 0.9 loss : 0.00700062096119\n",
      "step : 17100 training accuracy : 0.96 loss : 0.00219637051225\n",
      "step : 17200 training accuracy : 0.88 loss : 0.00866587877274\n",
      "step : 17300 training accuracy : 0.84 loss : 0.014322013855\n",
      "step : 17400 training accuracy : 0.92 loss : 0.00356389701366\n",
      "step : 17500 training accuracy : 0.96 loss : 0.00493948370218\n",
      "step : 17600 training accuracy : 0.92 loss : 0.00655121207237\n",
      "step : 17700 training accuracy : 0.94 loss : 0.00370453864336\n",
      "step : 17800 training accuracy : 0.98 loss : 0.00288629084826\n",
      "step : 17900 training accuracy : 0.96 loss : 0.00369074970484\n",
      "step : 18000 training accuracy : 0.94 loss : 0.00431859970093\n",
      "step : 18100 training accuracy : 0.88 loss : 0.0109995317459\n",
      "step : 18200 training accuracy : 0.98 loss : 0.00182157441974\n",
      "step : 18300 training accuracy : 0.9 loss : 0.0114486277103\n",
      "step : 18400 training accuracy : 0.92 loss : 0.00653666853905\n",
      "step : 18500 training accuracy : 0.9 loss : 0.00591499745846\n",
      "step : 18600 training accuracy : 0.96 loss : 0.00435351192951\n",
      "step : 18700 training accuracy : 0.92 loss : 0.0065709733963\n",
      "step : 18800 training accuracy : 0.94 loss : 0.0035469019413\n",
      "step : 18900 training accuracy : 0.94 loss : 0.00728590369225\n",
      "step : 19000 training accuracy : 0.92 loss : 0.00554104685783\n",
      "step : 19100 training accuracy : 0.88 loss : 0.0142450225353\n",
      "step : 19200 training accuracy : 0.86 loss : 0.0084530043602\n",
      "step : 19300 training accuracy : 0.92 loss : 0.0135305738449\n",
      "step : 19400 training accuracy : 0.92 loss : 0.00589145421982\n",
      "step : 19500 training accuracy : 0.94 loss : 0.00407301604748\n",
      "step : 19600 training accuracy : 0.94 loss : 0.00488094508648\n",
      "step : 19700 training accuracy : 0.98 loss : 0.00145003974438\n",
      "step : 19800 training accuracy : 0.92 loss : 0.0110414040089\n",
      "step : 19900 training accuracy : 0.94 loss : 0.0044971087575\n",
      "test accuracy 0.9092\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "\n",
    "\n",
    "n_iteration = 20000\n",
    "learning_rate = 0.01\n",
    "batch_size = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Initialize placeholders for input and output\n",
    "with tf.variable_scope('Placeholders') as scope:\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='X')  # inputs\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='Y')  # outputs\n",
    "\n",
    "# Define the first layer\n",
    "with tf.variable_scope('Layer1') as scope:\n",
    "    W_conv1 = weight_variable([3, 3, 1, 6])\n",
    "    b_conv1 = bias_variable([6])\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    h_conv1 = conv2d(x_image, W_conv1) + b_conv1\n",
    "\n",
    "# Define the second layer\n",
    "with tf.variable_scope('Layer2') as scope:\n",
    "    W_conv2 = weight_variable([3, 3, 6, 6])\n",
    "    b_conv2 = bias_variable([6])\n",
    "    h_conv2 = conv2d(h_conv1, W_conv2) + b_conv2\n",
    "\n",
    "# Define the last layer\n",
    "with tf.variable_scope('Layer3') as scope:\n",
    "    h_layer3 = tf.contrib.layers.flatten(h_conv2)\n",
    "    logits = tf.layers.dense(h_layer3, 10)\n",
    "    y_conv = tf.nn.softmax(logits)\n",
    "\n",
    "# Loss\n",
    "with tf.name_scope(\"loss\"):\n",
    "    entropy = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=y)\n",
    "    loss = tf.reduce_mean(entropy)\n",
    "    \n",
    "# Optimizer\n",
    "with tf.name_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "# Get infos on accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "\n",
    "# To display the loss and the training accuracy in Tensorboard \n",
    "with tf.name_scope(\"summary\"):\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    merge_summary = tf.summary.merge_all()\n",
    "\n",
    "# Print the logs for Tensorboard\n",
    "writer_train = tf.summary.FileWriter(\"./my_modelMNIST/\")\n",
    "\n",
    "# Run session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer_train.add_graph(sess.graph)\n",
    "    \n",
    "    for i in range(n_iteration):\n",
    "        total_loss = 0\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        _, l, summary= sess.run([optimizer, loss, merge_summary], feed_dict={x: batch[0], y: batch[1]})\n",
    "        total_loss += l\n",
    "        if i % 100 == 0:\n",
    "          train_accuracy = accuracy.eval(feed_dict={x: batch[0], y: batch[1]})\n",
    "          print('step :', i, 'training accuracy :', train_accuracy, 'loss :', total_loss/(len(batch[0])))\n",
    "        writer_train.add_summary(summary, i)\n",
    "\n",
    "    print_W_conv1 = sess.run(W_conv1)\n",
    "    print_W_conv1 = sess.run(W_conv2)\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the weights of your model\n",
    "And give an interpretation on what they are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_con1 [array([[ 4.84613419,  4.65626192],\n",
      "       [-2.7323525 ,  2.09388518]], dtype=float32)]\n",
      "W_con2 [array([[-4.24455166],\n",
      "       [ 3.93683577]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(\"W_con1\", print_W1)\n",
    "print(\"W_con2\", print_W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose one (tell me what you chose...)\n",
    "- Show how the gradients (show only one kernel) evolve for good and wrong prediction. (hard)\n",
    "- Initialize the kernels with values that make sense for you and show how they evolve. (easy) \n",
    "- When training is finished, show the 6+6=12 results of some convolved immages. (easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
