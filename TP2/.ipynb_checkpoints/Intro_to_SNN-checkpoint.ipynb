{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving to Shallow Neural Networks\n",
    "\n",
    "In this tutorial, you'll implement a shallow neural network to classify digits ranging from 0 to 9. The dataset you'll use is quite famous, it's called 'MNIST' http://yann.lecun.com/exdb/mnist/. A French guy put it up, he's very famous in the DL comunity, he's called Yann Lecun and is now both head of the Facebook AI reseach program and head of something in the University of New York...\n",
    "\n",
    "\n",
    "###Â First step\n",
    "\n",
    "As a first step, I invite you to discover what is MNIST. You might find [this notebook](https://nbviewer.jupyter.org/github/marc-moreaux/Deep-Learning-classes/blob/master/notebooks/dataset_MNIST.ipynb) to be usefull, but feel to browse the web.\n",
    "\n",
    "Once you get the idea, you can download the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset in this directory (does that work on Windows OS ?)\n",
    "#! wget http://deeplearning.net/data/mnist/mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as cPickle\n",
    "import gzip, numpy\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f, encoding='latin1') #added for Python 3\n",
    "f.close()\n",
    "\n",
    "def to_one_hot(y, n_classes=10): # You might want to use this as some point...\n",
    "    _y = np.zeros((len(y), n_classes))\n",
    "    _y[np.arange(len(y)), y] = 1\n",
    "    return _y\n",
    "\n",
    "X_train, y_train = train_set[0], train_set[1]\n",
    "X_valid, y_valid = valid_set[0], valid_set[1]\n",
    "X_test,  y_test  = test_set[0],  test_set[1]\n",
    "y_train = to_one_hot(y_train)\n",
    "y_valid = to_one_hot(y_valid)\n",
    "y_test = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# You can now implement a 2 layers NN\n",
    "\n",
    "Now that you have the data, you can build the a shallow neural network (SNN). I expect your SNN to have two layers. \n",
    "    - Layer 1 has 20 neurons with a sigmoid activation\n",
    "    - Layer 2 has 10 neurons with a softmax activation\n",
    "    - Loss is Negative Log Likelihood (wich is also the cross entropy)\n",
    "    \n",
    "You'll need to comment your work such that I understand that you understand what you are doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HELPER \n",
    "def softmax(Z):\n",
    "    \"\"\"Z is a vector eg. [1,2,3]\n",
    "    return: the vector softmax(Z) eg. [.09, .24, .67]\n",
    "    \"\"\"\n",
    "    return np.exp(Z) / np.exp(Z).sum(axis=0)\n",
    "\n",
    "n_hidden = 20    \n",
    "# Define the variables here (initialize the weights with the np.random.normal module):\n",
    "W1, b1 = np.random.normal(size=(X_train.shape[1], n_hidden)), np.random.normal(size=n_hidden)\n",
    "W2, b2 = np.random.normal(size=(n_hidden ,y_train.shape[1])), np.random.normal(size=y_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "\n",
    "def Pred(X, W1, b1, W2, b2):\n",
    "    \"\"\"Explanations ...\n",
    "    Here we do the vectorized version\n",
    "    Arguments:\n",
    "        X: An input image (as a vector)(shape is <50000,784>) \n",
    "        Layer1 : Contain W1 (784,20) and b1 (20)\n",
    "        Layer2 : Contain W2 (20,10) and b2 (10)\n",
    "    Returns : P, a matrix (50000, 10)\n",
    "    \"\"\"\n",
    "    A1 = np.dot(X, W1) + b1 # A1 = W1t*X + b1\n",
    "    A2 = sigmoid(A1)\n",
    "    Z = np.dot(A2, W2) + b2\n",
    "    P = softmax(Z.T).T # A2 = softmax(A1t*W2) + b2 \n",
    "    #We needed to transpose one time to apply the softmax and then another one to put the matrix in the right pass order\n",
    "    return P\n",
    "\n",
    "def loss(P, Y):\n",
    "    \"\"\"Explanations : \n",
    "    Here we do the vectorized version\n",
    "    Arguments:\n",
    "        P: The prediction vector corresponding to an image (X^s)\n",
    "        Y: The ground truth of an image\n",
    "    Returns: a scalar\n",
    "    \"\"\"\n",
    "    return -np.sum(np.multiply(Y, np.log(P)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Define Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dW1(W1, b1, W2, P, Y, X):\n",
    "    \"\"\"Explanations ??\n",
    "    Vectorized version\n",
    "    Returns: A vector which is the derivative of the loss with respect to W1\n",
    "    The formula that we need to implement is :\n",
    "    X*A2*(A-A2)*W2*(P-Y)\n",
    "    \"\"\"\n",
    "    A2 = sigmoid(np.dot(X, W1) + b1)\n",
    "    new_A2 = np.zeros(A2.shape[1])\n",
    "    # We do the following to have a vector of size 20 based on A2*(1-A2) if we use the matrix A2 as a all,\n",
    "    # it doesn't work\n",
    "    for i in range(A2.shape[1]):\n",
    "        new_A2[i] = np.matmul(A2[i], 1-A2[i])\n",
    "    dW1 = np.dot(X.T, np.dot((P-Y), np.multiply(W2.T, new_A2)))\n",
    "    return dW1\n",
    "\n",
    "\n",
    "def db1(W1, b1, W2, P, Y, X):\n",
    "    \"\"\"Explanations ??\n",
    "    Vectorized version\n",
    "    Arguments:\n",
    "        L is the loss af a sample (a scalar)\n",
    "    Returns: A scalar which is the derivative of the Loss with respect to b1\n",
    "    The formula that we need to implement is :\n",
    "    A2*(A-A2)*W2*(P-Y)\n",
    "    \"\"\"\n",
    "    A2 = sigmoid(np.dot(X, W1) + b1)\n",
    "    new_A2 = np.zeros(A2.shape[1])\n",
    "    for i in range(A2.shape[1]):\n",
    "        new_A2[i] = np.matmul(A2[i], 1-A2[i])\n",
    "    db1 = np.sum(np.dot((P-Y), np.multiply(W2.T, new_A2)), axis=0)\n",
    "    return db1\n",
    "\n",
    "def dW2(W1, b1, W2, P, Y, X):\n",
    "    \"\"\"\n",
    "    The formula that we need to implement is :\n",
    "    A2*(P-Y)\n",
    "    \"\"\"\n",
    "    A2 = sigmoid(np.dot(X, W1) + b1)\n",
    "    return np.dot(A2.T, (P-Y))\n",
    "\n",
    "\n",
    "def db2(P, Y):\n",
    "    \"\"\"\n",
    "    The formula that we need to implement is :\n",
    "    P-Y\n",
    "    \"\"\"\n",
    "    return np.sum(P-Y, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Train you model\n",
    "\n",
    "You may use Standard Gradient Descent (SGD) to train your model. (Experiment with many learning rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 4.08379457984\n",
      "Accuracy 0.1369\n",
      "Loss 4.64370226353\n",
      "Accuracy 0.0927\n",
      "Loss 3.24699558219\n",
      "Accuracy 0.1808\n",
      "Loss 2.49906779982\n",
      "Accuracy 0.3466\n",
      "Loss 2.22878569368\n",
      "Accuracy 0.3216\n",
      "Loss 1.83012316056\n",
      "Accuracy 0.4114\n",
      "Loss 1.45737038149\n",
      "Accuracy 0.5012\n",
      "Loss 1.49464879178\n",
      "Accuracy 0.53\n",
      "Loss 1.5260663479\n",
      "Accuracy 0.4656\n",
      "Loss 1.7326525182\n",
      "Accuracy 0.4303\n",
      "Loss 2.03984953014\n",
      "Accuracy 0.4136\n",
      "Loss 2.42895875938\n",
      "Accuracy 0.4121\n",
      "Loss 1.40088166299\n",
      "Accuracy 0.5507\n",
      "Loss 1.37330561038\n",
      "Accuracy 0.5672\n",
      "Loss 1.4125726401\n",
      "Accuracy 0.5524\n",
      "Loss 1.19836654098\n",
      "Accuracy 0.6121\n",
      "Loss 1.13808432584\n",
      "Accuracy 0.6228\n",
      "Loss 1.13271637765\n",
      "Accuracy 0.6281\n",
      "Loss 1.09664074298\n",
      "Accuracy 0.6414\n",
      "Loss 1.23683528443\n",
      "Accuracy 0.5896\n",
      "Loss 1.08955500856\n",
      "Accuracy 0.643\n",
      "Loss 1.13476486213\n",
      "Accuracy 0.6232\n",
      "Loss 1.0411162639\n",
      "Accuracy 0.6562\n",
      "Loss 1.09907590824\n",
      "Accuracy 0.6398\n",
      "Loss 0.99878013147\n",
      "Accuracy 0.6701\n",
      "Loss 1.03010662383\n",
      "Accuracy 0.6699\n",
      "Loss 0.91479986406\n",
      "Accuracy 0.7005\n",
      "Loss 0.899695102226\n",
      "Accuracy 0.7112\n",
      "Loss 0.874057471555\n",
      "Accuracy 0.7122\n",
      "Loss 0.858129261947\n",
      "Accuracy 0.7199\n",
      "Loss 0.845717887199\n",
      "Accuracy 0.7239\n",
      "Loss 0.833862369933\n",
      "Accuracy 0.7309\n",
      "Loss 0.823738729161\n",
      "Accuracy 0.7366\n",
      "Loss 0.813768743798\n",
      "Accuracy 0.7414\n",
      "Loss 0.804253879572\n",
      "Accuracy 0.7445\n",
      "Loss 0.794868603611\n",
      "Accuracy 0.7467\n",
      "Loss 0.786032961292\n",
      "Accuracy 0.7499\n",
      "Loss 0.778015167835\n",
      "Accuracy 0.7526\n",
      "Loss 0.770824598856\n",
      "Accuracy 0.7548\n",
      "Loss 0.764399542484\n",
      "Accuracy 0.7577\n",
      "Loss 0.758655732157\n",
      "Accuracy 0.7608\n",
      "Loss 0.753444819363\n",
      "Accuracy 0.7622\n",
      "Loss 0.748633719745\n",
      "Accuracy 0.7634\n",
      "Loss 0.744132932195\n",
      "Accuracy 0.7625\n",
      "Loss 0.739882201381\n",
      "Accuracy 0.764\n",
      "Loss 0.735834063\n",
      "Accuracy 0.7662\n",
      "Loss 0.731953922892\n",
      "Accuracy 0.7674\n",
      "Loss 0.728223746886\n",
      "Accuracy 0.7689\n",
      "Loss 0.724643890914\n",
      "Accuracy 0.7702\n",
      "Loss 0.721232596462\n",
      "Accuracy 0.7715\n",
      "Loss 0.71800987577\n",
      "Accuracy 0.7738\n",
      "Loss 0.714977058149\n",
      "Accuracy 0.7752\n",
      "Loss 0.712114157358\n",
      "Accuracy 0.7755\n",
      "Loss 0.70939253858\n",
      "Accuracy 0.7764\n",
      "Loss 0.706783640637\n",
      "Accuracy 0.777\n",
      "Loss 0.704260398318\n",
      "Accuracy 0.7786\n",
      "Loss 0.701799541826\n",
      "Accuracy 0.779\n",
      "Loss 0.699387386689\n",
      "Accuracy 0.7811\n",
      "Loss 0.697022234977\n",
      "Accuracy 0.7816\n",
      "Loss 0.694712082073\n",
      "Accuracy 0.7816\n",
      "Loss 0.692465231809\n",
      "Accuracy 0.7832\n",
      "Loss 0.69027494713\n",
      "Accuracy 0.7837\n",
      "Loss 0.688127890936\n",
      "Accuracy 0.7853\n",
      "Loss 0.686024358354\n",
      "Accuracy 0.7863\n",
      "Loss 0.683975232231\n",
      "Accuracy 0.7885\n",
      "Loss 0.681987851647\n",
      "Accuracy 0.7884\n",
      "Loss 0.680065234223\n",
      "Accuracy 0.7885\n",
      "Loss 0.678208247032\n",
      "Accuracy 0.7895\n",
      "Loss 0.676416057532\n",
      "Accuracy 0.7911\n",
      "Loss 0.674687889904\n",
      "Accuracy 0.7914\n",
      "Loss 0.673024534095\n",
      "Accuracy 0.7916\n",
      "Loss 0.671427247477\n",
      "Accuracy 0.7921\n",
      "Loss 0.669894772244\n",
      "Accuracy 0.7923\n",
      "Loss 0.668422257569\n",
      "Accuracy 0.7919\n",
      "Loss 0.667002454221\n",
      "Accuracy 0.7926\n",
      "Loss 0.665627361874\n",
      "Accuracy 0.7925\n",
      "Loss 0.664289551247\n",
      "Accuracy 0.792\n",
      "Loss 0.662983205604\n",
      "Accuracy 0.7926\n",
      "Loss 0.661704897967\n",
      "Accuracy 0.7919\n",
      "Loss 0.660453778346\n",
      "Accuracy 0.7919\n",
      "Loss 0.659230844053\n",
      "Accuracy 0.792\n",
      "Loss 0.658037673654\n",
      "Accuracy 0.7921\n",
      "Loss 0.656875481786\n",
      "Accuracy 0.792\n",
      "Loss 0.655744808634\n",
      "Accuracy 0.7917\n",
      "Loss 0.654645424661\n",
      "Accuracy 0.7916\n",
      "Loss 0.653575974219\n",
      "Accuracy 0.7914\n",
      "Loss 0.652533529088\n",
      "Accuracy 0.7917\n",
      "Loss 0.651513764984\n",
      "Accuracy 0.792\n",
      "Loss 0.650512069427\n",
      "Accuracy 0.792\n",
      "Loss 0.649524942029\n",
      "Accuracy 0.792\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00005\n",
    "tol = 0.001 # tolerance for convergence\n",
    "while True:\n",
    "\n",
    "    #Forward Propagation\n",
    "    P = Pred(X_train, W1, b1, W2, b2)\n",
    "    previous_loss = loss(P, y_train)/len(X_train)\n",
    "    #Backward Propagation\n",
    "    W1 -= learning_rate * dW1(W1, b1, W2, P, y_train, X_train)\n",
    "    b1 -= learning_rate * db1(W1, b1, W2, P, y_train, X_train)\n",
    "    W2 -= learning_rate * dW2(W1, b1, W2, P, y_train, X_train)\n",
    "    b2 -= learning_rate * db2(P, y_train)\n",
    "    new_loss = loss(Pred(X_train, W1, b1, W2, b2), y_train)/len(X_train)\n",
    "    print(\"Loss\" , new_loss)\n",
    "    print(\"Accuracy\" ,np.sum(np.equal(np.argmax(Pred(X_test, W1, b1, W2, b2), axis=1),np.argmax(y_test, axis = 1)))/len(y_test))\n",
    "    if (np.abs(previous_loss - new_loss) < tol):\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Test the accuracy of your model on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.650512069427\n",
      "Accuracy 0.792\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss\" , loss(P, y_train)/len(X_train))\n",
    "print(\"Accuracy\" ,np.sum(np.equal(np.argmax(Pred(X_test, W1, b1, W2, b2), axis=1),np.argmax(y_test, axis = 1)))/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# You can now go Deeper\n",
    "\n",
    "Build a deeper model trained with SGD (You don't need to use the biases here)\n",
    "    - Layer 1 has 10 neurons with a sigmoid activation\n",
    "    - Layer 2 has 10 neurons with a sigmoid activation\n",
    "    - Layer 3 has 10 neurons with a sigmoid activation\n",
    "    - Layer 4 has 10 neurons with a sigmoid activation\n",
    "    - Layer 5 has 10 neurons with a sigmoid activation\n",
    "    - Layer 6 has 10 neurons with a softmax activation\n",
    "    - Loss is Negative Log Likelihood\n",
    "\n",
    "Is it converging ? Why ? What's wrong ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
