{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colun or semicolun ?\n",
    "\n",
    "In this notebook, you are going to implement a logistic regression algrorithm.\n",
    "- 1st, you'll build a dataset\n",
    "- 2nd, you'll you are going do define a model\n",
    "- 3rd, a backpropagation method\n",
    "- 4th, a gradient descent method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Dataset\n",
    "\n",
    "We build a dataset to illustrate our purpose.\n",
    "\n",
    "The dataset we build is supposed to help us converting a paper scan into a ASCII string. Lets imagine that, when a paper is scaned, we can detect, with high confidence that we are over a colun or a semicolun. Our objective here is to detect wether it's one or the other.\n",
    "\n",
    "Therefore, our algorithm is fed with a vector $x_i \\in [0,1]^5$ which represent the intensity of the pen stroke writting on the paper.\n",
    "\n",
    "Here below, you have an example of 'perfect' strokes for $x_1$ an example of colun, and $x_2$ an example of semicolun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAD8CAYAAAAylrwMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYIjAAgz5H2i9JxUt9EElAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc878555e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x_1 = np.array([0,1,0,1,0])\n",
    "x_2 = np.array([0,1,0,1,1])\n",
    "\n",
    "def to_img(vec):\n",
    "    matrix = np.ones((5, 3))\n",
    "    matrix[:, 1] = 1-vec\n",
    "    return matrix\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].imshow(to_img(x_1), cmap='gray')\n",
    "axs[1].imshow(to_img(x_2), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever a sample $x_i$ belongs to the class *colun*, we'll label it with $y_i=0$.  \n",
    "Likewise, whenever a sample $x_i$ belongs to the class *semicolun*, we'll label it with $y_i=1$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_1 = 0\n",
    "y_2 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAByCAYAAABOU1q9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nchGRyGmQi4hE7v94VcPobONB/gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8786375f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [.0, 1., 0., 1., 0.],\n",
    "    [.0, .9, 0., .9, 0.],\n",
    "    [.2, .8, 0., .8, .2],\n",
    "    [.0, 1., 0., 1., 1.],\n",
    "    [.0, 1., 0., .5, .5],\n",
    "    [.2, .8, 0., .7, .7]])\n",
    "y = np.array([0,0,0,1,1,1])\n",
    "\n",
    "fig, axs = plt.subplots(1,6)\n",
    "for i in range(len(X)):\n",
    "    axs[i].imshow(to_img(X[i]), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Define a logistic regression model\n",
    "\n",
    "(You may want to read this : http://cs229.stanford.edu/notes/cs229-notes1.pdf).\n",
    "\n",
    "You're going to build a model which outputs a prediction value $p_i$ given an input $x_i$. This prediction $p_i$ will reflect the propability that your input $x_i$ belongs to class 1.\n",
    "$$\n",
    "\\begin{align}\n",
    "p_i &= P(Y=1 | W, x_i) \\\\\n",
    "p(x_i,W) &= P(Y=1 | W, x_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As $p_i$ is a probability, it must be in [0,1].\n",
    "\n",
    "The model we'll consider perform a weighted sum of its input:\n",
    "- Weighted sum : $ s = (W^t \\cdot X + b) $\n",
    "\n",
    "And then squizes the values between 0 and 1 (which is our prediction value):\n",
    "- prediction : $ p(s) = \\frac{1}{1 + e^{-s}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.88079708,  0.85814894,  0.88079708,  0.95257413,  0.88079708,\n",
       "        0.9168273 ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "\n",
    "W = np.ones(X.shape[1])\n",
    "b = 0\n",
    "s = np.dot(np.transpose(W), np.transpose(X)) + b\n",
    "p = sigmoid(s)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Compare these predicted values ($p_i$) with the true output ($y_i$)\n",
    "\n",
    "Overall, we would like to maximize the likelihood that we are right at predicting a label.  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\max \\text{likelihood} &= \\text{argmax}_w \\Pi_i P(Y | W, x_i) \\\\\n",
    "&= \\text{argmax}_w \\Pi_i \\big( P(Y=y_i | W, x_i) \\big) \\\\\n",
    "&= \\text{argmax}_w \\Pi_i \\big( P(Y=1 | W, x_i)^{y_i} \\cdot P(Y=0 | W, x_i)^{1-y_i} \\big) \\\\\n",
    "&= \\text{argmax}_w \\Pi_i \\big( P(Y=1 | W, x_i)^{y_i} \\cdot 1-P(Y=1 | W, x_i)^{1-y_i}\\big) \\\\\n",
    "&= \\text{argmax}_w \\Pi_i \\big( p_i^{y_i} \\cdot 1-p_i^{1-y_i}\\big) \\\\\n",
    "&= \\text{argmax}_w \\sum_i \\big( y_i \\ln(p_i) \\cdot (1-y_i) \\ln(1-p_i) \\big) \\\\\n",
    "&= \\text{argmin}_w - \\sum_i \\big( y_i \\ln(p_i) \\cdot (1-y_i) \\ln(1-p_i) \\big) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "And this term is going to be our **loss** that we want to reduce:\n",
    "$$\n",
    "L(x_i, W, y_i) = - \\sum_i \\big( y_i \\ln(p_i) \\cdot (1-y_i) \\ln(1-p_i) \\big)\n",
    "$$\n",
    "This is how you compare the prediction you made ($p_i$) to the true output you expected ($y_i$).\n",
    "\n",
    "#### In our example :\n",
    "In means of colun and semicolun : remember $x_0$, it's a colun, therefore it's label is $y_0=0$.  \n",
    "If your classifier is good you'ld expect it to predict it's a semicolun, hense have $p_i = $*\"Something small like 0.1\"*. \n",
    "\n",
    "The error for this one sample is going to be:\n",
    "$$\n",
    "\\begin{align}\n",
    "L(X, W, y) &= - \\sum_i \\big( y_i \\ln(p_i) \\cdot (1-y_i) \\ln(1-p_i) \\big) \\\\\n",
    "&= y_0 \\ln(p_0) \\cdot (1-y_0) \\ln(1-p_0) \\\\\n",
    "&= - 0 \\ln(.9) \\cdot (1-0) \\ln(1-.9) \\\\\n",
    "&= - \\ln(.1)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the minimum of the Loss function\n",
    "\n",
    "To reduce the error, we have to find the minimum of $L(x, W, y)$.  \n",
    "Hense, we derive it with respect to $W$ and find the 'zeros'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_loss(X, W, b, y):\n",
    "    parenthese = sigmoid(np.dot(W, X.T) + b) - y\n",
    "    grad_loss = np.dot(X.T, parenthese) / len(X)\n",
    "    return grad_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Stochastic gradient descent to solve this\n",
    "\n",
    "We are going to solve this with Stochastic Gradient Descent (SGD), meaning that we start with some values for $W$ and update this values such that our loss value disminushes.\n",
    "$$\n",
    "W = W + \\alpha \\frac{\\delta L(x, W, y)}{\\delta W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04677537  0.06220801  0.24693385  0.95670467  0.8095872   0.90214687]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "for i in range(10000):\n",
    "    W-= learning_rate * grad_loss(X, W, b, y)\n",
    "print(sigmoid(np.dot(np.transpose(W), np.transpose(X)) + b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
